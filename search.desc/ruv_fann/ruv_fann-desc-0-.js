searchState.loadedDescShard("ruv_fann", 0, "Pure Rust implementation of the Fast Artificial Neural …\nEnumeration of available training algorithms\nCascade Correlation Training for Dynamic Network Topology …\nComprehensive error handling system for ruv-FANN\nReturns the argument unchanged.\nIntegration utilities for combining all agent …\nCalls <code>U::from(self)</code>.\nI/O and serialization module for rUv-FANN\nMock types for testing I/O functionality\nHelper macros for error creation with context\nTraining algorithms for neural networks\nActivation functions available for neurons\nCosine activation: f(x) = cos(x * steepness) / 2 + 0.5 …\nSymmetric cosine: f(x) = cos(x * steepness) Output range: […\nElliott activation: f(x) = ((x * steepness) / 2) / (1 + |x …\nSymmetric Elliott: f(x) = (x * steepness) / (1 + |x * …\nGaussian activation: f(x) = exp(-x * steepness * x * …\nSymmetric gaussian: f(x) = exp(-x * steepness * x * …\nLinear activation function: f(x) = x * steepness\nBounded linear: f(x) = max(0, min(1, x * steepness)) …\nSymmetric bounded linear: f(x) = max(-1, min(1, x * …\nRectified Linear Unit (ReLU): f(x) = max(0, x) Output …\nLeaky ReLU: f(x) = x if x &gt; 0, 0.01 * x if x &lt;= 0 Output …\nSigmoid activation function: f(x) = 1 / (1 + exp(-2 * …\nSymmetric sigmoid (tanh): f(x) = tanh(steepness * x) …\nSine activation: f(x) = sin(x * steepness) / 2 + 0.5 …\nSymmetric sine: f(x) = sin(x * steepness) Output range: […\nHyperbolic tangent: alias for SigmoidSymmetric\nThreshold activation function: f(x) = 0 if x &lt; 0, 1 if x …\nSymmetric threshold: f(x) = -1 if x &lt; 0, 1 if x &gt;= 0 Note: …\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nReturns whether this activation function can be used …\nReturns the string name of the activation function\nReturns the output range of the activation function\nCandidate neuron for cascade correlation\nCascade correlation builder for easy configuration\nConfiguration for cascade correlation training\nCascade correlation specific errors\nPerformance metrics for cascade training\nA network that supports cascade correlation training\nState information for cascade training\nCascade correlation trainer\nTraining record for cascade correlation\nResult of cascade correlation training\nActivation function\nCalculate the derivative of the activation function\nBest correlation achieved\nBest error achieved\nBias weight\nBias gradient\nMomentum term for bias\nCalculate correlation with provided data\nHelper methods for network structure manipulation\nCalculate correlation between candidate output and …\nCalculate the output of this candidate neuron\nCalculate network residuals (errors) for candidate training\nActivation functions to try for candidates\nLearning rate for candidate training\nMaximum epochs for candidate training\nTarget correlation for stopping candidate training\nWeight range for candidate initialization\nConfiguration parameters\nConfiguration for cascade training\nCurrent correlation score\nCurrent epoch\nCurrent training error\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGenerate initial candidate neurons\nCurrent hidden neuron count\nNumber of hidden neurons added\nInstall a candidate neuron into the network\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nMaximum number of hidden neurons to add\nPerformance metrics\nMinimum correlation improvement to accept candidate\nMomentum coefficient\nCurrent network being trained\nThe underlying network\nCreate a new cascade trainer\nCreate a new cascade network from a base network\nCreate a new candidate neuron with random weights\nNumber of candidate neurons to train in parallel\nCurrent output value\nLearning rate for output training\nMaximum epochs for output training\nTarget error for stopping output training\nWhether to enable parallel candidate training\nPatience for early stopping (epochs without improvement)\nCalculate Pearson correlation coefficient\nRandom seed for reproducible results\nRandom number generator\nCurrent cascade training state\nActivation steepness\nMain cascade training loop\nGenerate and train candidate neurons\nTrain candidates in parallel\nTrain candidates sequentially\nTrain output weights for one epoch\nTrain output weights using standard backpropagation\nTrain a single candidate neuron\nTrain single candidate with provided data (thread-safe)\nTraining data\nTraining history\nTraining history\nUpdate candidate weights (simplified for parallel)\nUpdate weights using gradient descent with optional …\nWhether to use momentum\nWhether to use weight decay\nConfiguration validation\nTraining data validation\nVerbose logging\nWeight decay coefficient\nGradient for weight updates\nMomentum terms for weights\nWeights connecting to all previous layers and inputs\nRepresents a connection between two neurons with a weight\nReturns the argument unchanged.\nIndex of the source neuron\nCalls <code>U::from(self)</code>.\nCreates a new connection between two neurons\nSets the weight to a specific value\nIndex of the destination neuron\nUpdates the weight of the connection\nThe weight of the connection\nAbort the entire process\nActivation function problems\nLearning algorithm failures\nCandidate neuron generation issues\nCandidate selection problems\nCandidate training failures\nCascade correlation specific errors\nCascade correlation error categories\nCascade parameter validation\nFANN compatibility errors\nNeuron connection issues\nConvergence problems\nCorrelation calculation issues\nTraining data I/O problems\nContains the error value\nComprehensive error category enum for uniform handling\nError context for providing additional debugging …\nProfessional error logging and debugging facilities\nUse fallback implementation\nFile reading/writing issues\nFormat compatibility issues\nGradient calculation issues\nInput data validation\nI/O and serialization errors\nI/O error categories\nEpoch and iteration errors\nLayer configuration problems\nLearning rate problems\nMemory allocation and management errors\nNetwork configuration and topology errors\nNetwork configuration validation\nNetwork error categories for detailed classification\nNetwork export/import errors\nContains the success value\nOutput data validation\nOutput training problems\nParallel processing and concurrency errors\nPerformance and optimization errors\nForward propagation errors\nError recovery context\nError recovery strategies\nReset to a known good state\nRetry the operation with the same parameters\nRetry with modified parameters\nMain error type for all ruv-FANN operations\nComprehensive result type for all ruv-FANN operations\nSerialization/deserialization problems\nSkip the problematic operation\nStop criteria issues\nInvalid network topology or structure\nNetwork topology modification errors\nTraining and learning algorithm errors\nTraining error categories\nTraining parameter validation\nData validation and format errors\nValidation error for detailed parameter checking\nValidation error categories\nWeight and bias configuration issues\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nBenchmark result for a specific test\nIndividual compatibility test\nFANN compatibility validator\nIntegration test configuration\nIntegration test suite errors\nIntegration test result\nComprehensive integration test suite\nPerformance regression detector\nCalculate overall scores\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGenerate test datasets\nGenerate test networks for integration testing\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nLoad baseline metrics for performance comparison\nMaximum test duration per component\nCreate a new integration test suite\nPerformance regression threshold (percentage)\nRandom seed for reproducible tests\nRun the complete integration test suite\nRun a basic network functionality test\nWhether to run performance benchmarks\nRun cascade integration test\nRun performance benchmarks\nRun stress tests\nWhether to run stress tests\nRun training integration test\nTest basic network functionality across all implementations\nTest cascade correlation integration\nTest cross-agent compatibility\nTest FANN compatibility\nWhether to run FANN compatibility tests\nTest I/O system integration\nWhether to test parallel execution\nTest parallel execution\nTest training algorithm integration\nVerbose output\nBinary format (using bincode)\nCompressed binary format\nCompressed FANN format\nDecompression error\nDOT format for visualization\nDOT format exporter for network visualization\nContains the error value\nNative FANN format (text-based)\nFANN file format reader\nFANN file format writer\nSupported file formats\nInvalid file format\nInvalid network structure\nInvalid training data\nI/O error (file not found, permission denied, etc.)\nError types for I/O operations\nResult type for I/O operations\nJSON format\nContains the success value\nParse error\nSerialization error\nTraining data file format reader\nStreaming reader for large training datasets\nTraining data file format writer\nBinary serialization support using bincode\nCompress data using gzip\nCompression support for file formats\nDecompress gzip data\nDOT format export for network visualization\nError types for I/O operations\nFANN native file format reader and writer\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nJSON serialization support\nGraph layout direction\nRead binary data from a reader\nRead JSON data from a reader\nShow neuron indices\nShow weights on edges\nStreaming I/O for large datasets\nTraining data file format reader and writer\nWrite binary data to a writer\nWrite JSON data to a writer\nBinary format configuration\nBinary reader with configuration\nBinary writer with configuration\nCreate a config optimized for size (variable length …\nCreate a config optimized for speed (fixed length encoding)\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nUtilities for binary format inspection\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nUse little endian byte order\nCreate a new binary reader with default config\nCreate a new binary writer with default config\nCreate a new binary config with default settings\nRead data from a reader\nRead binary data from a reader\nRead data with size limit to prevent memory exhaustion\nGet the size of serialized data without writing\nUse variable length encoding for integers\nCreate a new binary reader with custom config\nCreate a new binary writer with custom config\nWrite data to a writer\nWrite binary data to a writer\nGet the size of data when serialized\nCheck if data can be serialized without errors\nCompression wrapper for readers\nCompression wrapper for writers\nCompression configuration\nUtilities for compression analysis\nCreate a config optimized for compression ratio\nCompress data from bytes to bytes\nCompress data using gzip\nDecompress data from bytes to bytes\nDecompress gzip data\nCreate a config optimized for speed\nUse faster compression algorithm\nFinish compression and return the inner writer\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet a mutable reference to the inner reader\nGet a mutable reference to the inner writer\nGet a reference to the inner reader\nGet a reference to the inner writer\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nConsume the reader and return the inner reader\nCompression level (0-9, where 9 is best compression)\nCreate a new compressed reader\nCreate a new compressed writer with default compression\nCreate a new compression config with default settings\nCreate a new compressed writer with config\nCreate a new compressed writer with custom compression …\nCreate a config with custom compression level\nCompression statistics\nCalculate compression ratio\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalculate space savings percentage\nTest compression effectiveness on data\nBottom to top\nDOT format exporter for network visualization\nGraph layout direction\nLeft to right (default for neural networks)\nRight to left\nTop to bottom\nExport a neural network to DOT format\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nGraph layout direction\nCreate a new DOT exporter with default settings\nShow neuron indices\nShow weights on edges\nCreate a new DOT exporter with custom settings\nDecompression error\nContains the error value\nInvalid file format\nInvalid network structure\nInvalid training data\nI/O error (file not found, permission denied, etc.)\nError types for I/O operations\nResult type for I/O operations\nContains the success value\nParse error\nSerialization error\nFANN file format reader\nFANN file format writer\nCreate a new FANN reader\nCreate a new FANN writer\nRead a neural network from a FANN format file\nWrite a neural network to FANN format\nJSON format configuration\nJSON reader with configuration\nJSON writer with configuration\nCreate a compact JSON config (no pretty printing)\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nInclude null values in serialization\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCreate a new JSON reader with default config\nCreate a new JSON writer with default config\nCreate a new JSON config with default settings\nCreate a pretty JSON config (with indentation)\nUse pretty printing (indented, human-readable)\nRead data from a reader\nRead JSON data from a reader\nRead JSON data from a reader with custom options\nCreate a new JSON reader with custom config\nCreate a new JSON writer with custom config\nWrite data to a writer\nWrite JSON data to a writer\nWrite JSON data to a writer with custom options\nBuffered reader wrapper for streaming\nStatistics from streaming operations\nStreaming reader for training data\nCalculate average bytes per sample\nGet the number of bytes in the internal buffer\nGet the buffer size\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nUtilities for memory-efficient streaming\nCreate a new streaming reader with default buffer size\nCreate a new buffered stream reader\nCalculate total parameters (inputs + outputs) per sample\nRead training data in batches\nRead training data with a callback for each sample\nCreate a new streaming reader with custom buffer size\nCreate a new buffered stream reader with custom buffer size\nEstimate memory usage for batch processing\nCalculate optimal batch size for given memory limit\nTraining data file format reader\nStreaming reader for large training datasets\nTraining data file format writer\nCreate a new training data reader\nCreate a new training data writer\nCreate a new streaming reader\nRead training data from a FANN data format file\nRead training data with a callback for each sample\nWrite training data to FANN data format\nRepresents a layer of neurons in the neural network\nGets a reference to the bias neuron if it exists\nGets a mutable reference to the bias neuron if it exists\nCalculates outputs for all neurons in the layer based on …\nConnects all neurons in this layer to all neurons in the …\nReturns the argument unchanged.\nGets the output values of all neurons in the layer\nChecks if the layer has a bias neuron\nCalls <code>U::from(self)</code>.\nThe neurons in this layer\nCreates a new layer with the specified number of neurons\nReturns the number of regular neurons (excluding bias)\nResets all neurons in the layer\nSets the activation function for all neurons in the layer …\nSets the activation steepness for all neurons in the layer …\nSets the values of neurons in the layer (used for input …\nReturns the number of neurons in the layer (including bias …\nCreates a new layer with a bias neuron\nMock network structure for testing\nMock training data structure for testing\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nA feedforward neural network\nBuilder for creating neural networks with a fluent API\nErrors that can occur during network operations\nBuilds the network\nSets the connection rate (0.0 to 1.0)\nConnection rate (1.0 = fully connected, 0.0 = no …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nDeserialize a network from bytes\nAlias for total_connections for compatibility\nGets all weights in the network as a flat vector\nAdds a hidden layer with default activation (Sigmoid)\nAdds a hidden layer with specific activation function\nAdds an input layer to the network\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nThe layers of the network\nCreate layers from a slice of layer sizes\nCreates a new network builder\nCreates a new network with the specified layer sizes\nReturns the number of input neurons (excluding bias)\nReturns the number of layers in the network\nReturns the number of output neurons\nAdds an output layer with default activation (Sigmoid)\nAdds an output layer with specific activation function\nRandomizes all weights in the network within the given …\nResets all neurons in the network\nRuns a forward pass through the network\nRun batch inference on multiple inputs\nSets the activation function for all neurons in a specific …\nSets the activation function for all hidden layers\nSets the activation function for the output layer\nSets the activation steepness for all hidden layers\nSets the activation steepness for the output layer\nSets the training algorithm (placeholder for API …\nSets all weights in the network from a flat vector\nSerialize the network to bytes\nReturns the total number of connections in the network\nReturns the total number of neurons in the network\nTrain the network with the given data\nRepresents a single neuron in the neural network\nThe activation function to use\nThe steepness parameter for the activation function\nAdds a connection from another neuron to this neuron\nCalculates the neuron’s output based on inputs and …\nClears all connections\nIncoming connections to this neuron\nReturns the argument unchanged.\nGets the weight of a specific connection by index\nCalls <code>U::from(self)</code>.\nWhether this is a bias neuron\nCreates a new neuron with the specified activation …\nCreates a new bias neuron with a constant output value of …\nResets the neuron’s sum and value to zero (except for …\nSets the weight of a specific connection by index\nSets the neuron’s output value directly (used for input …\nThe sum of inputs multiplied by weights\nThe output value after applying the activation function\nBatch backpropagation Accumulates gradients over entire …\nBit fail based stop criteria\nTrait for error/loss functions\nExponential decay learning rate schedule\nIncremental (online) backpropagation Updates weights after …\nLearning rate schedule trait\nMean Absolute Error (MAE)\nMean Squared Error (MSE)\nMSE-based stop criteria\nOptions for parallel training\nQuickprop trainer An advanced batch training algorithm …\nRPROP (Resilient Propagation) trainer An adaptive learning …\nStep decay learning rate schedule\nStop criteria trait\nTanh Error Function\nMain trait for training algorithms\nCallback function type for training progress\nError types for training operations\nTraining state that can be saved and restored\nBackpropagation training algorithms\nBatch size for parallel processing\nCalculate the error between actual and desired outputs\nCalculate the current error\nCall the callback if set\nCount bit fails\nCalculate the derivative of the error function\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nHelper functions for forward propagation and gradient …\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nNumber of threads to use (0 = use all available cores)\nWhether to use parallel error calculation\nWhether to use parallel gradient computation\nQuickprop training algorithm\nRestore training state\nResilient Propagation (RPROP) training algorithm\nSave training state\nSet a callback function\nTrain for one epoch\nBatch backpropagation Accumulates gradients over entire …\nIncremental (online) backpropagation Updates weights after …\nSimple network representation for training algorithms\nApply weight and bias updates back to the real Network\nCalculate gradients using backpropagation on simplified …\nForward propagation through the simplified network\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nConvert a real Network to a simplified representation for …\nActivation function that works with our simplified …\nSigmoid derivative\nQuickprop trainer An advanced batch training algorithm …\nRPROP (Resilient Propagation) trainer An adaptive learning …")